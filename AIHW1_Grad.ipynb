{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIHW1-Grad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdviUQHYgx9nbodgPcOJLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbjeon2k/pytorchbookExcercise/blob/master/AIHW1_Grad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnALiqu64zU4"
      },
      "source": [
        "#https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\n",
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#input data given in HW1\n",
        "\n",
        "#x_i = [1, x_i0, x_i1]\n",
        "x = np.array(\n",
        "    [[1,1,2],\n",
        "     [1,2,1],\n",
        "     [1,3,2],\n",
        "     [1,3,3],\n",
        "     [1,4,5]])\n",
        "y = np.array([6,4,7,10,10])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kj582JLJ43a",
        "outputId": "c3c91daf-a593-40bf-a72b-950ac28163e6"
      },
      "source": [
        "\n",
        "\n",
        "# theta_0 = [0,0,0]\n",
        "a = 0\n",
        "b = 0\n",
        "c = 0\n",
        "\n",
        "def get_loss():\n",
        "    ret = 0 \n",
        "    #get squared loss\n",
        "    for t in range(5):\n",
        "        x_t = x[t]\n",
        "        y_pred = a*x_t[0] + b*x_t[1] + c*x_t[2]\n",
        "        #print(f'y_pred{t} = {y_pred}')\n",
        "        #L2 squared loss\n",
        "        ret += (y_pred - y[t])*(y_pred - y[t])\n",
        "    return ret\n",
        "\n",
        "learning_rate = 1e-2\n",
        "for t in range(5):\n",
        "    # 순전파 단계: 예측값 y를 계산합니다\n",
        "    # a = theta_0\n",
        "    # b = theta_1\n",
        "    # c = theta_2\n",
        "    # y = ax[0] + bx[1] + cx[2]\n",
        "    datanum = t\n",
        "    print(f'{t}th learning')\n",
        "    print(f'start with parameters {a}, {b}, {c}, learning rate {learning_rate}')\n",
        "\n",
        "    # 손실(loss)을 계산하고 출력합니다\n",
        "    loss = get_loss()\n",
        "    print(f'loss {loss}')     \n",
        "\n",
        "    # 손실에 따른 a, b, c의 변화도(gradient)를 계산하고 역전파합니다.\n",
        "    grad_y_pred = np.zeros(3)\n",
        "    for i in range(5):\n",
        "        x_i = x[i]\n",
        "        y_hat = a * x_i[0] + b * x_i[1] + c * x_i[2]\n",
        "        for j in range(3):\n",
        "            #grad[j] = 2*(y_hat - y)*x_i[j] for all i\n",
        "            print(f'i: {i}, j:{j}')\n",
        "            #print(f\"j {j} yhat {y_hat}, y[i] {y[i]}, x_i[j] {x_i[j]}\")\n",
        "            tmp= 2.0 * (y_hat - y[i]) * x_i[j]\n",
        "            print(f'2.0 * (y_hat - y[i]) * x_i[j] = 2*({y_hat} - {y[i]})*{x_i[j]} = {tmp}')\n",
        "            grad_y_pred[j] += tmp\n",
        "    \n",
        "    print(grad_y_pred)\n",
        "    grad_a = grad_y_pred[0] #x_t[0] is always 1\n",
        "    grad_b = grad_y_pred[1]\n",
        "    grad_c = grad_y_pred[2]\n",
        "    print(f'grad: grad_a {grad_a}, grad_b {grad_b},  grad_c {grad_c}')\n",
        "\n",
        "    # 가중치를 갱신합니다.\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    print(f'updated parameters: {a}, {b}, {c}')\n",
        "    print(\"\\n\")\n",
        "\n",
        "#final parameters:\n",
        "print(f'final parameters: {a}, {b}, {c}\\n')\n",
        "print(f'final loss: {get_loss()}')\n",
        "#get final loss for data 1~5\n",
        "for t in range(5):\n",
        "    x_t = x[t]\n",
        "    #print(f'x[0] {x_t[0]}, x[1] {x_t[1]}, x[0] {x_t[2]}')\n",
        "    y_pred = a * x_t[0] + b * x_t[1] + c * x_t[2]\n",
        "    print(f'{t}th loss, y_pred {y_pred}, y_label {y[t]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0th learning\n",
            "start with parameters 0, 0, 0, learning rate 0.01\n",
            "loss 301\n",
            "i: 0, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 6)*1 = -12.0\n",
            "i: 0, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 6)*1 = -12.0\n",
            "i: 0, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 6)*2 = -24.0\n",
            "i: 1, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 4)*1 = -8.0\n",
            "i: 1, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 4)*2 = -16.0\n",
            "i: 1, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 4)*1 = -8.0\n",
            "i: 2, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 7)*1 = -14.0\n",
            "i: 2, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 7)*3 = -42.0\n",
            "i: 2, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 7)*2 = -28.0\n",
            "i: 3, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 10)*1 = -20.0\n",
            "i: 3, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 10)*3 = -60.0\n",
            "i: 3, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 10)*3 = -60.0\n",
            "i: 4, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 10)*1 = -20.0\n",
            "i: 4, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 10)*4 = -80.0\n",
            "i: 4, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(0 - 10)*5 = -100.0\n",
            "[ -74. -210. -220.]\n",
            "grad: grad_a -74.0, grad_b -210.0,  grad_c -220.0\n",
            "updated parameters: 0.74, 2.1, 2.2\n",
            "\n",
            "\n",
            "1th learning\n",
            "start with parameters 0.74, 2.1, 2.2, learning rate 0.01\n",
            "loss 147.18000000000004\n",
            "i: 0, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(7.24 - 6)*1 = 2.4800000000000004\n",
            "i: 0, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(7.24 - 6)*1 = 2.4800000000000004\n",
            "i: 0, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(7.24 - 6)*2 = 4.960000000000001\n",
            "i: 1, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(7.140000000000001 - 4)*1 = 6.280000000000001\n",
            "i: 1, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(7.140000000000001 - 4)*2 = 12.560000000000002\n",
            "i: 1, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(7.140000000000001 - 4)*1 = 6.280000000000001\n",
            "i: 2, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(11.440000000000001 - 7)*1 = 8.880000000000003\n",
            "i: 2, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(11.440000000000001 - 7)*3 = 26.640000000000008\n",
            "i: 2, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(11.440000000000001 - 7)*2 = 17.760000000000005\n",
            "i: 3, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(13.64 - 10)*1 = 7.280000000000001\n",
            "i: 3, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(13.64 - 10)*3 = 21.840000000000003\n",
            "i: 3, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(13.64 - 10)*3 = 21.840000000000003\n",
            "i: 4, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(20.14 - 10)*1 = 20.28\n",
            "i: 4, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(20.14 - 10)*4 = 81.12\n",
            "i: 4, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(20.14 - 10)*5 = 101.4\n",
            "[ 45.2  144.64 152.24]\n",
            "grad: grad_a 45.2, grad_b 144.64000000000001,  grad_c 152.24\n",
            "updated parameters: 0.288, 0.6536, 0.6776\n",
            "\n",
            "\n",
            "2th learning\n",
            "start with parameters 0.288, 0.6536, 0.6776, learning rate 0.01\n",
            "loss 74.69095680000001\n",
            "i: 0, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(2.2968 - 6)*1 = -7.4064\n",
            "i: 0, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(2.2968 - 6)*1 = -7.4064\n",
            "i: 0, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(2.2968 - 6)*2 = -14.8128\n",
            "i: 1, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(2.2728 - 4)*1 = -3.4543999999999997\n",
            "i: 1, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(2.2728 - 4)*2 = -6.908799999999999\n",
            "i: 1, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(2.2728 - 4)*1 = -3.4543999999999997\n",
            "i: 2, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.6039999999999996 - 7)*1 = -6.792000000000001\n",
            "i: 2, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.6039999999999996 - 7)*3 = -20.376\n",
            "i: 2, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.6039999999999996 - 7)*2 = -13.584000000000001\n",
            "i: 3, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(4.281599999999999 - 10)*1 = -11.436800000000002\n",
            "i: 3, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(4.281599999999999 - 10)*3 = -34.3104\n",
            "i: 3, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(4.281599999999999 - 10)*3 = -34.3104\n",
            "i: 4, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(6.2904 - 10)*1 = -7.4192\n",
            "i: 4, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(6.2904 - 10)*4 = -29.6768\n",
            "i: 4, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(6.2904 - 10)*5 = -37.096000000000004\n",
            "[ -36.5088  -98.6784 -103.2576]\n",
            "grad: grad_a -36.5088, grad_b -98.6784,  grad_c -103.2576\n",
            "updated parameters: 0.653088, 1.640384, 1.710176\n",
            "\n",
            "\n",
            "3th learning\n",
            "start with parameters 0.653088, 1.640384, 1.710176, learning rate 0.01\n",
            "loss 40.500869474304004\n",
            "i: 0, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.713824 - 6)*1 = -0.5723520000000004\n",
            "i: 0, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.713824 - 6)*1 = -0.5723520000000004\n",
            "i: 0, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.713824 - 6)*2 = -1.1447040000000008\n",
            "i: 1, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.644032 - 4)*1 = 3.2880640000000003\n",
            "i: 1, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.644032 - 4)*2 = 6.576128000000001\n",
            "i: 1, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.644032 - 4)*1 = 3.2880640000000003\n",
            "i: 2, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(8.994592 - 7)*1 = 3.9891840000000016\n",
            "i: 2, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(8.994592 - 7)*3 = 11.967552000000005\n",
            "i: 2, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(8.994592 - 7)*2 = 7.978368000000003\n",
            "i: 3, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(10.704768000000001 - 10)*1 = 1.4095360000000028\n",
            "i: 3, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(10.704768000000001 - 10)*3 = 4.228608000000008\n",
            "i: 3, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(10.704768000000001 - 10)*3 = 4.228608000000008\n",
            "i: 4, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(15.765504 - 10)*1 = 11.531008\n",
            "i: 4, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(15.765504 - 10)*4 = 46.124032\n",
            "i: 4, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(15.765504 - 10)*5 = 57.65504\n",
            "[19.64544  68.323968 72.005376]\n",
            "grad: grad_a 19.645440000000004, grad_b 68.32396800000001,  grad_c 72.00537600000001\n",
            "updated parameters: 0.4566336, 0.9571443199999999, 0.9901222399999998\n",
            "\n",
            "\n",
            "4th learning\n",
            "start with parameters 0.4566336, 0.9571443199999999, 0.9901222399999998, learning rate 0.01\n",
            "loss 24.34675902165321\n",
            "i: 0, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.3940223999999994 - 6)*1 = -5.211955200000001\n",
            "i: 0, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.3940223999999994 - 6)*1 = -5.211955200000001\n",
            "i: 0, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.3940223999999994 - 6)*2 = -10.423910400000002\n",
            "i: 1, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.3610444799999994 - 4)*1 = -1.277911040000001\n",
            "i: 1, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.3610444799999994 - 4)*2 = -2.555822080000002\n",
            "i: 1, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(3.3610444799999994 - 4)*1 = -1.277911040000001\n",
            "i: 2, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.3083110399999995 - 7)*1 = -3.383377920000001\n",
            "i: 2, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.3083110399999995 - 7)*3 = -10.150133760000003\n",
            "i: 2, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(5.3083110399999995 - 7)*2 = -6.766755840000002\n",
            "i: 3, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(6.298433279999999 - 10)*1 = -7.403133440000001\n",
            "i: 3, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(6.298433279999999 - 10)*3 = -22.209400320000004\n",
            "i: 3, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(6.298433279999999 - 10)*3 = -22.209400320000004\n",
            "i: 4, j:0\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(9.235822079999998 - 10)*1 = -1.5283558400000032\n",
            "i: 4, j:1\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(9.235822079999998 - 10)*4 = -6.113423360000013\n",
            "i: 4, j:2\n",
            "2.0 * (y_hat - y[i]) * x_i[j] = 2*(9.235822079999998 - 10)*5 = -7.641779200000016\n",
            "[-18.80473344 -46.24073472 -48.3197568 ]\n",
            "grad: grad_a -18.804733440000007, grad_b -46.24073472000002,  grad_c -48.31975680000002\n",
            "updated parameters: 0.6446809344000001, 1.4195516672000001, 1.473319808\n",
            "\n",
            "\n",
            "final parameters: 0.6446809344000001, 1.4195516672000001, 1.473319808\n",
            "\n",
            "final loss: 16.687121282889176\n",
            "0th loss, y_pred 5.0108722176, y_label 6\n",
            "1th loss, y_pred 4.9571040768, y_label 4\n",
            "2th loss, y_pred 7.849975552, y_label 7\n",
            "3th loss, y_pred 9.32329536, y_label 10\n",
            "4th loss, y_pred 13.689486643200002, y_label 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B1NAjTF8l4l",
        "outputId": "0b8b6e05-51f0-452a-e912-bf3b3b823c5a"
      },
      "source": [
        "#get pseudoinverse\n",
        "\n",
        "xT = np.transpose(x)\n",
        "print(\"xT:\")\n",
        "print(xT)\n",
        "\n",
        "xTx = np.matmul( xT, x)\n",
        "print(\"xTx:\")\n",
        "print(xTx)\n",
        "\n",
        "xTx_inverse = np.linalg.inv(xTx)\n",
        "print(\"xTx_inverse:\")\n",
        "print(xTx_inverse)\n",
        "\n",
        "print(\"theta_star:\")\n",
        "theta_star = np.linalg.multi_dot([xTx_inverse,xT,y])\n",
        "print(theta_star)\n",
        "\n",
        "def get_loss2():\n",
        "    ret = 0 \n",
        "    #get squared loss\n",
        "    for t in range(5):\n",
        "        x_t = x[t]\n",
        "        y_pred = theta_star[0]*x_t[0] + theta_star[1]*x_t[1] + theta_star[2]*x_t[2]\n",
        "        ret += (y_pred - y[t])*(y_pred - y[t])\n",
        "    return ret\n",
        "\n",
        "print(f'Loss: {get_loss2()}')\n",
        "\n",
        "#https://www.wolframalpha.com/input/?i=matrix+calculator&assumption=%7B%22F%22%2C+%22MatricesOperations%22%2C+%22theMatrix3%22%7D+-%3E%22%5B6%2C4%2C7%2C10%2C10%5D%22&assumption=%7B%22F%22%2C+%22MatricesOperations%22%2C+%22theMatrix2%22%7D+-%3E%22%5B%5B1%2C1%2C1%2C1%2C1%5D+%2C%5B1%2C2%2C3%2C3%2C4%5D+%2C%5B2%2C1%2C2%2C3%2C5%5D%5D%22&assumption=%7B%22F%22%2C+%22MatricesOperations%22%2C+%22theMatrix1%22%7D+-%3E%22%5B%5B+1.5%2C-0.5%2C0+%5D+%2C+%5B-0.5%2C0.44230769%2C-0.25%5D+%2C+%5B+0%2C-0.25%2C+0.25%5D%5D%22&assumption=%22FSelect%22+-%3E+%7B%7B%22MatricesOperations%22%7D%7D&assumption=%7B%22FP%22%2C+%22MatricesOperations%22%2C+%22matricesop%22%7D+-%3E+%22matrixmult%22\n",
        "\n",
        "for i in range(5):\n",
        "    x_i = x[i]\n",
        "    y_hat = theta_star[0]*x_i[0] + theta_star[1]*x_i[1] + theta_star[2]*x_i[2]\n",
        "    #Loss = get_loss2()\n",
        "    print(f'y_hat_analytic[{i+1}]: {y_hat}, y_label[{i+1}]: {y[i]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xT:\n",
            "[[1 1 1 1 1]\n",
            " [1 2 3 3 4]\n",
            " [2 1 2 3 5]]\n",
            "xTx:\n",
            "[[ 5 13 13]\n",
            " [13 39 39]\n",
            " [13 39 43]]\n",
            "xTx_inverse:\n",
            "[[ 1.5        -0.5         0.        ]\n",
            " [-0.5         0.44230769 -0.25      ]\n",
            " [ 0.         -0.25        0.25      ]]\n",
            "theta_star:\n",
            "[3.         0.44230769 1.25      ]\n",
            "Loss: 6.057692307692308\n",
            "y_hat_analytic[1]: 5.9423076923077005, y_label[1]: 6\n",
            "y_hat_analytic[2]: 5.134615384615394, y_label[2]: 4\n",
            "y_hat_analytic[3]: 6.826923076923087, y_label[3]: 7\n",
            "y_hat_analytic[4]: 8.076923076923087, y_label[4]: 10\n",
            "y_hat_analytic[5]: 11.01923076923078, y_label[5]: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loi9FIN0KavU",
        "outputId": "881e7f4b-6831-42d3-9e58-f0458062b1ae"
      },
      "source": [
        "w1 = 0\n",
        "w2 = 0\n",
        "w3 = 0\n",
        "\n",
        "def get_loss3():\n",
        "    ret = 0 \n",
        "    #get squared loss\n",
        "    for idx in range(5):\n",
        "        x_t = x[idx]\n",
        "        y_pred = w1*x_t[0] + w2*x_t[1] + w3*x_t[2]\n",
        "        ret += (y_pred - y[idx])*(y_pred - y[idx])\n",
        "    return ret\n",
        "\n",
        "learning_rate = 1e-2\n",
        "for t in range(300):\n",
        "    # 순전파 단계: 예측값 y를 계산합니다\n",
        "    # a = theta_0\n",
        "    # b = theta_1\n",
        "    # c = theta_2\n",
        "    # y = ax[0] + bx[1] + cx[2]\n",
        "    #datanum = t\n",
        "    #print(f'{t}th learning')\n",
        "    #print(f'start with parameters {a}, {b}, {c}, learning rate {learning_rate}')\n",
        "\n",
        "    # 손실(loss)을 계산하고 출력합니다\n",
        "    loss = get_loss3()\n",
        "    if(t % 100 == 0):\n",
        "        print(f'loss: {loss}')\n",
        "\n",
        "    # 손실에 따른 a, b, c의 변화도(gradient)를 계산하고 역전파합니다.\n",
        "    grad_y_pred = np.zeros(3)\n",
        "    for i in range(5):\n",
        "        x_i = x[i]\n",
        "        y_hat = w1 * x_i[0] + w2 * x_i[1] + w3 * x_i[2]\n",
        "        for j in range(3):\n",
        "            #grad[j] = 2*(y_hat - y)*x_i[j] for all i\n",
        "            #print(f'i: {i}, j:{j}')\n",
        "            #print(f\"j {j} yhat {y_hat}, y[i] {y[i]}, x_i[j] {x_i[j]}\")\n",
        "            tmp= 2.0 * (y_hat - y[i]) * x_i[j]\n",
        "            #print(f'2.0 * (y_hat - y[i]) * x_i[j] = 2*({y_hat} - {y[i]})*{x_i[j]} = {tmp}')\n",
        "            grad_y_pred[j] += tmp\n",
        "    \n",
        "    #print(grad_y_pred)\n",
        "    grad_a = grad_y_pred[0] #x_t[0] is always 1\n",
        "    grad_b = grad_y_pred[1]\n",
        "    grad_c = grad_y_pred[2]\n",
        "    #print(f'grad: grad_a {grad_a}, grad_b {grad_b},  grad_c {grad_c}')\n",
        "\n",
        "    # 가중치를 갱신합니다.\n",
        "    w1 -= learning_rate * grad_a\n",
        "    w2 -= learning_rate * grad_b\n",
        "    w3 -= learning_rate * grad_c\n",
        "    print(f'updated parameters: {w1}, {w2}, {w3}')\n",
        "    #print(\"\\n\")\n",
        "\n",
        "\n",
        "#get final loss for data 1~5\n",
        "print(f'w1: {w1}, w2: {w2},w3: {w3}')\n",
        "print(f'Loss: {get_loss3()}')\n",
        "for i in range(5):\n",
        "    x_i = x[i]\n",
        "    y_hat = w1*x_i[0] + w2*x_i[1] + w3*x_i[2]  \n",
        "    print(f'y_hat_numerical[{i+1}]: {y_hat}, y_label[{i+1}]: {y[i]}')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 301\n",
            "updated parameters: 0.74, 2.1, 2.2\n",
            "updated parameters: 0.288, 0.6536, 0.6776\n",
            "updated parameters: 0.653088, 1.640384, 1.710176\n",
            "updated parameters: 0.4566336, 0.9571443199999999, 0.9901222399999998\n",
            "updated parameters: 0.6446809344000001, 1.4195516672000001, 1.473319808\n",
            "updated parameters: 0.568066257408, 1.0954948735999999, 1.1313974297599998\n",
            "updated parameters: 0.6722676327936001, 1.3108216500531202, 1.3562124118323202\n",
            "updated parameters: 0.6516120134240256, 1.1557454972561407, 1.192639266088755\n",
            "updated parameters: 0.7158707736119502, 1.2545862583568754, 1.2960688859023892\n",
            "updated parameters: 0.7211133587433463, 1.1789488446955418, 1.2167459613688645\n",
            "updated parameters: 0.766121373292266, 1.222817422692035, 1.2632748624558485\n",
            "updated parameters: 0.7831252418245898, 1.1844738832206967, 1.2238693339880424\n",
            "updated parameters: 0.8186434811678587, 1.202353610923487, 1.2438395149717894\n",
            "updated parameters: 0.840768920318301, 1.1814756676215281, 1.2234544104720875\n",
            "updated parameters: 0.8714102079821308, 1.1870302874257497, 1.2311326774385418\n",
            "updated parameters: 0.895546816319202, 1.1742965207562484, 1.2199082965739572\n",
            "updated parameters: 0.9234988821814284, 1.1739745909956956, 1.221993703087488\n",
            "updated parameters: 0.9481972375016579, 1.165009612243641, 1.2152692280884343\n",
            "updated parameters: 0.9745050152651525, 1.1618608350341912, 1.2148989126319099\n",
            "updated parameters: 0.999096979345451, 1.1546169278856928, 1.2104630924728585\n",
            "updated parameters: 1.0242664761176825, 1.1500892973762056, 1.2090984145655426\n",
            "updated parameters: 1.0484510234010598, 1.1436135982710445, 1.205894842295138\n",
            "updated parameters: 1.0727337265137462, 1.1383997485451465, 1.204209405185629\n",
            "updated parameters: 1.09638197389237, 1.1322538397415676, 1.2017267439671997\n",
            "updated parameters: 1.1199088247388533, 1.126689671236713, 1.2000244359449692\n",
            "updated parameters: 1.1429722743977306, 1.120676373202899, 1.198009183035558\n",
            "updated parameters: 1.1658168023359587, 1.1149288479934927, 1.196420923183307\n",
            "updated parameters: 1.188284181596395, 1.1089636578682398, 1.1947420592033895\n",
            "updated parameters: 1.2104922769981319, 1.1031193113373063, 1.1933183479361849\n",
            "updated parameters: 1.232369257887211, 1.097169945084469, 1.1919035138484526\n",
            "updated parameters: 1.2539732327759303, 1.0912766400661151, 1.1906579277222227\n",
            "updated parameters: 1.2752729218733694, 1.0853346366694698, 1.1894632901077995\n",
            "updated parameters: 1.2962981687239423, 1.0794212940961236, 1.1883928843258293\n",
            "updated parameters: 1.3170366654618404, 1.0734887110587752, 1.1873888705424147\n",
            "updated parameters: 1.337504827699347, 1.0675746643897686, 1.1864837142300149\n",
            "updated parameters: 1.3576991664882687, 1.0616578738645073, 1.1856482265663524\n",
            "updated parameters: 1.3776296637274184, 1.055757332241487, 1.1848958268180239\n",
            "updated parameters: 1.3972968759992037, 1.0498641556059398, 1.1842109840370347\n",
            "updated parameters: 1.41670765209211, 1.0439883589246268, 1.1835983086327588\n",
            "updated parameters: 1.4358643533179787, 1.0381267686859175, 1.1830488537034287\n",
            "updated parameters: 1.4547722561649508, 1.032285051359553, 1.1825632280807898\n",
            "updated parameters: 1.4734344778939665, 1.0264626067931983, 1.182135725267972\n",
            "updated parameters: 1.4918554637686656, 1.0206629435330543, 1.1817652039863902\n",
            "updated parameters: 1.5100385990367435, 1.0148865678880346, 1.1814476120224593\n",
            "updated parameters: 1.5279878523563408, 1.009135871808296, 1.181181106980924\n",
            "updated parameters: 1.5457066526355097, 1.0034117867400558, 1.18096253335421\n",
            "updated parameters: 1.5631986641474496, 0.9977160873812958, 1.1807898313271132\n",
            "updated parameters: 1.5804672588685182, 0.9920498181103999, 1.1806603755500484\n",
            "updated parameters: 1.5975158826299498, 0.9864143797494356, 1.18057210714508\n",
            "updated parameters: 1.6143478077743807, 0.9808107904879263, 1.1805227493119645\n",
            "updated parameters: 1.630966306648971, 0.9752401994226725, 1.1805103383017534\n",
            "updated parameters: 1.6473745361757233, 0.9697035402688878, 1.1805328520838285\n",
            "updated parameters: 1.6635756205464447, 0.9642017748280809, 1.1805884584763153\n",
            "updated parameters: 1.6795725978326572, 0.9587357315085763, 1.1806753384787054\n",
            "updated parameters: 1.6953684598526981, 0.9533062214820055, 1.1807918013738383\n",
            "updated parameters: 1.7109661279249089, 0.9479139640927459, 1.1809361998746717\n",
            "updated parameters: 1.7263684725008894, 0.9425596429376839, 1.1811069827296359\n",
            "updated parameters: 1.7415783025772973, 0.9372438720669433, 1.1813026532405244\n",
            "updated parameters: 1.756598375739626, 0.9319672236570212, 1.1815217925713604\n",
            "updated parameters: 1.7714313939462842, 0.9267302133065809, 1.1817630388152112\n",
            "updated parameters: 1.78608000899999, 0.9215333142255492, 1.1820250966289625\n",
            "updated parameters: 1.800546821277818, 0.9163769514190329, 1.1823067260921292\n",
            "updated parameters: 1.814834382997134, 0.9112615094280938, 1.1826067460138199\n",
            "updated parameters: 1.828945198282523, 0.9061873306041464, 1.182924027508767\n",
            "updated parameters: 1.8428817253449135, 0.901154719722618, 1.1832574944265373\n",
            "updated parameters: 1.8566463771316417, 0.8961639440965995, 1.1836061192463958\n",
            "updated parameters: 1.8702415229492988, 0.8912152366348364, 1.183968922244921\n",
            "updated parameters: 1.883669489345632, 0.886308796741808, 1.1843449685722989\n",
            "updated parameters: 1.896932561429401, 0.8814447925669404, 1.1847333669116473\n",
            "updated parameters: 1.910032983822028, 0.8766233622019977, 1.1851332671937729\n",
            "updated parameters: 1.922972961796925, 0.8718446154795694, 1.1855438590958427\n",
            "updated parameters: 1.9357546622276254, 0.8671086352435475, 1.1859643701321534\n",
            "updated parameters: 1.9483802146071807, 0.8624154788713182, 1.1863940641493518\n",
            "updated parameters: 1.9608517119610884, 0.8577651795173284, 1.186832239663414\n",
            "updated parameters: 1.9731712117779867, 0.8531577474464666, 1.187278228419479\n",
            "updated parameters: 1.9853407368750422, 0.8485931712087524, 1.1877313939082066\n",
            "updated parameters: 1.9973622762571286, 0.8440714188300134, 1.1881911300168109\n",
            "updated parameters: 2.0092377859312416, 0.839592438902637, 1.1886568596880895\n",
            "updated parameters: 2.0209691897045285, 0.8351561616597476, 1.189128033670153\n",
            "updated parameters: 2.0325583799483016, 0.8307624999792477, 1.1896041292960409\n",
            "updated parameters: 2.0440072183418962, 0.8264113503579642, 1.1900846493310742\n",
            "updated parameters: 2.0553175365885568, 0.8221025938316213, 1.1905691208582454\n",
            "updated parameters: 2.0664911371103356, 0.8178360968605004, 1.1910570942184648\n",
            "updated parameters: 2.077529793718771, 0.8136117121702202, 1.1915481419907075\n",
            "updated parameters: 2.0884352522650524, 0.8094292795578162, 1.1920418580190468\n",
            "updated parameters: 2.0992092312685626, 0.8052886266589494, 1.1925378564786564\n",
            "updated parameters: 2.1098534225259287, 0.8011895696817904, 1.193035770983205\n",
            "updated parameters: 2.120369491700437, 0.7971319141063526, 1.1935352537291106\n",
            "updated parameters: 2.1307590788931727, 0.7931154553525777, 1.1940359746770068\n",
            "updated parameters: 2.1410237991961636, 0.7891399794172768, 1.1945376207675453\n",
            "updated parameters: 2.1511652432284936, 0.7852052634821128, 1.1950398951709778\n",
            "updated parameters: 2.1611849776558407, 0.7813110764932938, 1.1955425165684805\n",
            "updated parameters: 2.171084545694195, 0.7774571797145914, 1.1960452184642996\n",
            "updated parameters: 2.180865467598264, 0.7736433272545656, 1.1965477485271299\n",
            "updated parameters: 2.190529241135197, 0.7698692665692947, 1.1970498679596886\n",
            "updated parameters: 2.2000773420441417, 0.7661347389415366, 1.1975513508951554\n",
            "updated parameters: 2.2095112244821875, 0.7624394799374399, 1.1980519838194463\n",
            "updated parameters: 2.218832321457178, 0.7587832198416998, 1.1985515650181506\n",
            "updated parameters: 2.228042045247899, 0.7551656840721502, 1.199049904047149\n",
            "updated parameters: 2.2371417878120914, 0.7515865935746431, 1.1995468212258698\n",
            "loss: 6.456646564374878\n",
            "updated parameters: 2.246132921182749, 0.7480456651990993, 1.2000421471522564\n",
            "updated parameters: 2.2550167978531217, 0.7445426120575271, 1.2005357222385038\n",
            "updated parameters: 2.2637947511508414, 0.7410771438648114, 1.2010273962667077\n",
            "updated parameters: 2.272468095601562, 0.7376489672630077, 1.2015170279635674\n",
            "updated parameters: 2.2810381272824967, 0.7342577861298731, 1.2020044845933475\n",
            "updated parameters: 2.2895061241662096, 0.730903301872312, 1.2024896415683186\n",
            "updated parameters: 2.2978733464550247, 0.7275852137054056, 1.2029723820759468\n",
            "updated parameters: 2.3061410369063706, 0.7243032189176443, 1.2034525967221097\n",
            "updated parameters: 2.3143104211493974, 0.7210570131229799, 1.2039301831896765\n",
            "updated parameters: 2.322382707993167, 0.7178462905002645, 1.204405045911787\n",
            "updated parameters: 2.330359089726717, 0.714670744020641, 1.2048770957592205\n",
            "updated parameters: 2.338240742411281, 0.7115300656634025, 1.2053462497412444\n",
            "updated parameters: 2.346028826164945, 0.7084239466208448, 1.2058124307193872\n",
            "updated parameters: 2.35372448543999, 0.7053520774925781, 1.2062755671335694\n",
            "updated parameters: 2.361328849293193, 0.7023141484697857, 1.2067355927400913\n",
            "updated parameters: 2.368843031649306, 0.6993098495098514, 1.2071924463609498\n",
            "updated parameters: 2.376268131557967, 0.696338870501807, 1.2076460716440294\n",
            "updated parameters: 2.3836052334442526, 0.6934009014229834, 1.2080964168336834\n",
            "updated parameters: 2.390855407353094, 0.6904956324872777, 1.208543434551283\n",
            "updated parameters: 2.398019709187759, 0.6876227542853959, 1.2089870815852986\n",
            "updated parameters: 2.4050991809426026, 0.6847819579174368, 1.2094273186905156\n",
            "updated parameters: 2.412094850930275, 0.6819729351181574, 1.2098641103959948\n",
            "updated parameters: 2.419007734003568, 0.6791953783752472, 1.2102974248214051\n",
            "updated parameters: 2.4258388317720816, 0.6764489810409309, 1.2107272335013763\n",
            "updated parameters: 2.432589132813874, 0.67373343743719, 1.2111535112175253\n",
            "updated parameters: 2.4392596128822603, 0.6710484429549048, 1.2115762358378381\n",
            "updated parameters: 2.445851235107921, 0.6683936941471775, 1.2119953881630838\n",
            "updated parameters: 2.452364950196461, 0.6657688888171144, 1.212410951779974\n",
            "updated parameters: 2.4588016966215718, 0.6631737261003057, 1.2128229129207673\n",
            "updated parameters: 2.4651624008139357, 0.66060790654226, 1.2132312603290603\n",
            "updated parameters: 2.4714479773459987, 0.658071132171007, 1.2136359851314824\n",
            "updated parameters: 2.4776593291127518, 0.6555631065651056, 1.2140370807150624\n",
            "updated parameters: 2.483797347508633, 0.6530835349172591, 1.214434542610011\n",
            "updated parameters: 2.4898629126006795, 0.6506321240937439, 1.2148283683776948\n",
            "updated parameters: 2.4958568932980376, 0.648208582689845, 1.2152185575035803\n",
            "updated parameters: 2.5017801475179433, 0.6458126210814835, 1.2156051112949324\n",
            "updated parameters: 2.507633522348281, 0.6434439514732139, 1.215988032783068\n",
            "updated parameters: 2.5134178542068195, 0.641102287942761, 1.2163673266299697\n",
            "updated parameters: 2.5191339689972274, 0.6387873464822581, 1.2167429990390692\n",
            "updated parameters: 2.5247826822619595, 0.6364988450363437, 1.2171150576700294\n",
            "updated parameters: 2.5303647993321063, 0.6342365035372632, 1.2174835115573466\n",
            "updated parameters: 2.535881115474297, 0.6320000439371201, 1.2178483710326156\n",
            "updated parameters: 2.541332416034736, 0.629789190237409, 1.2182096476502953\n",
            "updated parameters: 2.5467194765804595, 0.6276036685159683, 1.218567354116831\n",
            "updated parameters: 2.552043063037886, 0.6254432069514655, 1.2189215042229817\n",
            "updated parameters: 2.557303931828741, 0.6233075358455463, 1.219272112779224\n",
            "updated parameters: 2.5625028300034267, 0.6211963876427528, 1.2196191955540925\n",
            "updated parameters: 2.5676404953719043, 0.6191094969483225, 1.2199627692153348\n",
            "updated parameters: 2.5727176566321632, 0.6170466005439748, 1.2203028512737601\n",
            "updated parameters: 2.577735033496336, 0.6150074374017792, 1.2206394600296637\n",
            "updated parameters: 2.5826933368145273, 0.6129917486962063, 1.2209726145217177\n",
            "updated parameters: 2.5875932686964145, 0.6109992778144484, 1.2213023344782223\n",
            "updated parameters: 2.592435522630679, 0.6090297703650974, 1.2216286402706136\n",
            "updated parameters: 2.597220783602326, 0.6070829741852665, 1.2219515528691334\n",
            "updated parameters: 2.60194972820795, 0.6051586393462299, 1.2222710938005663\n",
            "updated parameters: 2.6066230247689877, 0.6032565181576618, 1.222587285107953\n",
            "updated parameters: 2.611241333443029, 0.6013763651705455, 1.2229001493122005\n",
            "updated parameters: 2.615805306333212, 0.5995179371788161, 1.223209709375495\n",
            "updated parameters: 2.62031558759577, 0.5976809932198184, 1.2235159886664577\n",
            "updated parameters: 2.624772813545761, 0.5958652945736228, 1.2238190109269456\n",
            "updated parameters: 2.6291776127610373, 0.5940706047612817, 1.2241188002404488\n",
            "updated parameters: 2.633530606184484, 0.5922966895420622, 1.2244153810019933\n",
            "updated parameters: 2.637832407224581, 0.5905433169097333, 1.224708777889505\n",
            "updated parameters: 2.642083621854321, 0.5888102570879363, 1.2249990158365476\n",
            "updated parameters: 2.646284848708523, 0.5870972825247154, 1.225286120006403\n",
            "updated parameters: 2.65043667917958, 0.5854041678862272, 1.2255701157674024\n",
            "updated parameters: 2.6545396975116784, 0.5837306900497052, 1.2258510286694884\n",
            "updated parameters: 2.65859448089352, 0.5820766280956979, 1.2261288844219218\n",
            "updated parameters: 2.662601599549587, 0.5804417632996394, 1.2264037088721096\n",
            "updated parameters: 2.6665616168299735, 0.5788258791227826, 1.226675527985484\n",
            "updated parameters: 2.6704750892988267, 0.5772287612025415, 1.226944367826404\n",
            "updated parameters: 2.674342566821418, 0.5756501973422691, 1.2272102545400194\n",
            "updated parameters: 2.6781645926498814, 0.5740899775005155, 1.2274732143350642\n",
            "updated parameters: 2.6819417035076425, 0.5725478937797941, 1.2277332734675377\n",
            "updated parameters: 2.685674429672572, 0.5710237404148883, 1.227990458225229\n",
            "updated parameters: 2.689363295058884, 0.5695173137607282, 1.2282447949130506\n",
            "updated parameters: 2.6930088172978133, 0.5680284122798709, 1.2284963098391493\n",
            "updated parameters: 2.696611507817087, 0.5665568365296036, 1.2287450293017501\n",
            "updated parameters: 2.7001718719192263, 0.5651023891487051, 1.2289909795767116\n",
            "updated parameters: 2.7036904088586953, 0.5636648748438813, 1.2292341869057508\n",
            "updated parameters: 2.7071676119179213, 0.5622441003759074, 1.229474677485317\n",
            "updated parameters: 2.7106039684822107, 0.5608398745454928, 1.229712477456077\n",
            "updated parameters: 2.7139999601135814, 0.5594520081788936, 1.2299476128929916\n",
            "updated parameters: 2.717356062623533, 0.558080314113292, 1.2301801097959506\n",
            "updated parameters: 2.7206727461447766, 0.5567246071819641, 1.2304099940809468\n",
            "updated parameters: 2.723950475201942, 0.5553847041992516, 1.2306372915717585\n",
            "updated parameters: 2.7271897087812853, 0.5540604239453588, 1.2308620279921252\n",
            "updated parameters: 2.730390900399411, 0.552751587150987, 1.2310842289583834\n",
            "updated parameters: 2.7335544981710336, 0.5514580164818312, 1.2313039199725568\n",
            "updated parameters: 2.7366809448757894, 0.5501795365229398, 1.231521126415861\n",
            "updated parameters: 2.739770678024122, 0.5489159737629699, 1.2317358735426223\n",
            "updated parameters: 2.742824129922256, 0.5476671565783362, 1.231948186474579\n",
            "updated parameters: 2.7458417277362726, 0.5464329152172758, 1.2321580901955522\n",
            "updated parameters: 2.74882389355531, 0.5452130817838391, 1.2323656095464715\n",
            "updated parameters: 2.751771044453898, 0.5440074902218164, 1.232570769220731\n",
            "updated parameters: 2.754683592553446, 0.542815976298616, 1.2327735937598723\n",
            "updated parameters: 2.757561945082894, 0.5416383775890992, 1.2329741075495657\n",
            "updated parameters: 2.760406504438552, 0.5404745334593881, 1.2331723348158894\n",
            "updated parameters: 2.7632176682431244, 0.5393242850506482, 1.2333682996218782\n",
            "updated parameters: 2.765995829403955, 0.5381874752628651, 1.233562025864345\n",
            "loss: 6.09535935433093\n",
            "updated parameters: 2.768741376170485, 0.5370639487386131, 1.2337535372709452\n",
            "updated parameters: 2.771454692190951, 0.5359535518468316, 1.233942857397488\n",
            "updated parameters: 2.774136156568333, 0.5348561326666149, 1.2341300096254724\n",
            "updated parameters: 2.776786143915557, 0.5337715409710204, 1.23431501715984\n",
            "updated parameters: 2.7794050244099773, 0.5326996282109046, 1.234497903026937\n",
            "updated parameters: 2.7819931638471407, 0.5316402474987942, 1.2346786900726716\n",
            "updated parameters: 2.7845509236938457, 0.5305932535927944, 1.2348574009608582\n",
            "updated parameters: 2.7870786611405114, 0.5295585028805454, 1.2350340581717405\n",
            "updated parameters: 2.789576729152866, 0.5285358533632295, 1.2352086840006853\n",
            "updated parameters: 2.7920454765229614, 0.5275251646396307, 1.2353813005570318\n",
            "updated parameters: 2.7944852479195332, 0.5265262978902642, 1.2355519297631028\n",
            "updated parameters: 2.7968963839377046, 0.5255391158615591, 1.2357205933533495\n",
            "updated parameters: 2.799279221148058, 0.5245634828501273, 1.2358873128736496\n",
            "updated parameters: 2.80163409214507, 0.5235992646870863, 1.2360521096807167\n",
            "updated parameters: 2.8039613255949343, 0.5226463287224817, 1.2362150049416547\n",
            "updated parameters: 2.8062612462827654, 0.5217045438097723, 1.2363760196336129\n",
            "updated parameters: 2.808534175159209, 0.5207737802904129, 1.2365351745435644\n",
            "updated parameters: 2.810780429386454, 0.5198539099785162, 1.2366924902681826\n",
            "updated parameters: 2.8130003223836666, 0.518944806145613, 1.236847987213825\n",
            "updated parameters: 2.8151941638718463, 0.5180463435054983, 1.2370016855966042\n",
            "updated parameters: 2.817362259918115, 0.5171583981991783, 1.2371536054425558\n",
            "updated parameters: 2.8195049129794527, 0.5162808477799157, 1.2373037665878888\n",
            "updated parameters: 2.8216224219458783, 0.5154135711983706, 1.2374521886793126\n",
            "updated parameters: 2.8237150821830928, 0.5145564487878495, 1.2375988911744464\n",
            "updated parameters: 2.8257831855745867, 0.5137093622496546, 1.237743893342296\n",
            "updated parameters: 2.827827020563221, 0.5128721946385406, 1.2378872142637982\n",
            "updated parameters: 2.8298468721922907, 0.5120448303482789, 1.2380288728324325\n",
            "updated parameters: 2.8318430221460766, 0.5112271550973285, 1.2381688877548875\n",
            "updated parameters: 2.8338157487898927, 0.5104190559146201, 1.238307277551788\n",
            "updated parameters: 2.8357653272096375, 0.5096204211254496, 1.2384440605584746\n",
            "updated parameters: 2.8376920292508534, 0.508831140337483, 1.2385792549258299\n",
            "updated parameters: 2.839596123557307, 0.5080511044268771, 1.2387128786211576\n",
            "updated parameters: 2.8414778756090873, 0.5072802055245101, 1.2388449494290983\n",
            "updated parameters: 2.8433375477602403, 0.5065183370023328, 1.2389754849525931\n",
            "updated parameters: 2.8451753992759357, 0.5057653934598281, 1.239104502613881\n",
            "updated parameters: 2.8469916863691775, 0.5050212707105918, 1.2392320196555342\n",
            "updated parameters: 2.848786662237067, 0.5042858657690273, 1.2393580531415271\n",
            "updated parameters: 2.850560577096616, 0.5035590768371575, 1.239482619958335\n",
            "updated parameters: 2.8523136782201264, 0.5028408032915531, 1.2396057368160638\n",
            "updated parameters: 2.854046209970133, 0.5021309456703791, 1.2397274202496047\n",
            "updated parameters: 2.8557584138339243, 0.5014294056605573, 1.2398476866198145\n",
            "updated parameters: 2.857450528457635, 0.500736086085047, 1.2399665521147192\n",
            "updated parameters: 2.8591227896799323, 0.5000508908902443, 1.240084032750739\n",
            "updated parameters: 2.8607754305652833, 0.499373725133495, 1.2402001443739306\n",
            "updated parameters: 2.862408681436824, 0.49870449497072955, 1.2403149026612508\n",
            "updated parameters: 2.8640227699088268, 0.49804310764421067, 1.2404283231218318\n",
            "updated parameters: 2.865617920918773, 0.4973894714704026, 1.2405404210982771\n",
            "updated parameters: 2.867194356759039, 0.4967434958279515, 1.2406512117679638\n",
            "updated parameters: 2.8687522971081973, 0.49610509114578716, 1.2407607101443623\n",
            "updated parameters: 2.8702919590619387, 0.4954741688913392, 1.2408689310783654\n",
            "updated parameters: 2.8718135571636214, 0.4948506415588656, 1.2409758892596225\n",
            "updated parameters: 2.8733173034344524, 0.49423442265790335, 1.2410815992178905\n",
            "updated parameters: 2.874803407403301, 0.4936254267018266, 1.2411860753243824\n",
            "updated parameters: 2.876272076136156, 0.4930235691965253, 1.2412893317931306\n",
            "updated parameters: 2.87772351426523, 0.4924287666291931, 1.241391382682348\n",
            "updated parameters: 2.8791579240177065, 0.4918409364572314, 1.2414922418957983\n",
            "updated parameters: 2.880575505244148, 0.4912599970972645, 1.2415919231841674\n",
            "updated parameters: 2.8819764554465612, 0.4906858679142691, 1.2416904401464386\n",
            "updated parameters: 2.8833609698061213, 0.4901184692108112, 1.2417878062312655\n",
            "updated parameters: 2.8847292412105694, 0.48955772221639987, 1.2418840347383528\n",
            "updated parameters: 2.8860814602812765, 0.4890035490769447, 1.2419791388198296\n",
            "updated parameters: 2.8874178153999877, 0.48845587284432895, 1.2420731314816273\n",
            "updated parameters: 2.88873849273524, 0.4879146174660864, 1.2421660255848546\n",
            "updated parameters: 2.8900436762684714, 0.4873797077751901, 1.24225783384717\n",
            "updated parameters: 2.891333547819811, 0.4868510694799467, 1.242348568844153\n",
            "updated parameters: 2.8926082870735637, 0.48632862915399827, 1.2424382430106722\n",
            "updated parameters: 2.893868071603393, 0.4858123142264289, 1.242526868642249\n",
            "updated parameters: 2.8951130768971973, 0.48530205297197787, 1.242614457896418\n",
            "updated parameters: 2.896343476381695, 0.48479777450135786, 1.2427010227940845\n",
            "updated parameters: 2.8975594414467105, 0.48429940875167204, 1.2427865752208718\n",
            "updated parameters: 2.898761141469178, 0.48380688647694303, 1.242871126928473\n",
            "updated parameters: 2.8999487438368523, 0.48332013923873235, 1.2429546895359844\n",
            "updated parameters: 2.901122413971741, 0.4828390993968716, 1.2430372745312448\n",
            "updated parameters: 2.9022823153532564, 0.4823637001002882, 1.2431188932721617\n",
            "updated parameters: 2.903428609541094, 0.48189387527793054, 1.2431995569880312\n",
            "updated parameters: 2.9045614561978343, 0.481429559629796, 1.2432792767808543\n",
            "updated parameters: 2.905681013111282, 0.480970688618052, 1.243358063626642\n",
            "updated parameters: 2.906787436216533, 0.4805171984582574, 1.243435928376716\n",
            "updated parameters: 2.9078808796177866, 0.4800690261106797, 1.243512881759001\n",
            "updated parameters: 2.908961495609891, 0.479626109271704, 1.2435889343793052\n",
            "updated parameters: 2.9100294346996396, 0.4791883863653451, 1.2436640967226018\n",
            "updated parameters: 2.9110848456268092, 0.4787557965348402, 1.2437383791542886\n",
            "updated parameters: 2.9121278753849547, 0.4783282796343492, 1.2438117919214546\n",
            "updated parameters: 2.9131586692419504, 0.47790577622073416, 1.2438843451541233\n",
            "updated parameters: 2.9141773707602927, 0.4774882275454384, 1.2439560488664976\n",
            "updated parameters: 2.91518412181716, 0.47707557554645225, 1.2440269129581916\n",
            "updated parameters: 2.9161790626242365, 0.47666776284036866, 1.2440969472154526\n",
            "updated parameters: 2.917162331747299, 0.47626473271452646, 1.2441661613123742\n",
            "updated parameters: 2.918134066125575, 0.47586642911924615, 1.244234564812104\n",
            "updated parameters: 2.9190944010908666, 0.4754727966601435, 1.244302167168033\n",
            "updated parameters: 2.920043470386454, 0.47508378059054057, 1.2443689777249873\n",
            "updated parameters: 2.9209814061857715, 0.4746993268039508, 1.2444350057203986\n",
            "updated parameters: 2.9219083391108636, 0.47431938182665767, 1.2445002602854736\n",
            "updated parameters: 2.922824398250623, 0.47394389281037075, 1.2445647504463486\n",
            "updated parameters: 2.923729711178814, 0.4735728075249677, 1.2446284851252378\n",
            "updated parameters: 2.924624403971879, 0.4732060743513157, 1.2446914731415668\n",
            "updated parameters: 2.9255086012265417, 0.47284364227417874, 1.2447537232131045\n",
            "updated parameters: 2.926382426077194, 0.47248546087519705, 1.2448152439570745\n",
            "updated parameters: 2.927246000213084, 0.472131480325955, 1.2448760438912665\n",
            "updated parameters: 2.928099443895298, 0.47178165138112044, 1.2449361314351306\n",
            "w1: 2.928099443895298, w2: 0.47178165138112044,w3: 1.2449361314351306\n",
            "Loss: 6.0612490006838815\n",
            "y_hat_numerical[1]: 5.889753358146679, y_label[1]: 6\n",
            "y_hat_numerical[2]: 5.1165988780926694, y_label[2]: 4\n",
            "y_hat_numerical[3]: 6.83331666090892, y_label[3]: 7\n",
            "y_hat_numerical[4]: 8.078252792344053, y_label[4]: 10\n",
            "y_hat_numerical[5]: 11.039906706595431, y_label[5]: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAJdvs5uRPvQ",
        "outputId": "5756de8b-aa5f-4381-cff2-2e9040d25795"
      },
      "source": [
        "#prediction for [1,5,3]\n",
        "x_test = np.array([1,5,3])\n",
        "\n",
        "y_numeric = a* x_test[0] + b * x_test[1] + c * x_test[2]\n",
        "y_converge = w1 * x_test[0] + w2 * x_test[1] + w3 * x_test[2]\n",
        "y_analytic = theta_star[0]*x_test[0] + theta_star[1]*x_test[1] + theta_star[2]*x_test[2]\n",
        "print(f'theta 5: {a},{b},{c}')\n",
        "print(f'theta 300: {w1},{w2},{w3}')\n",
        "print(f'analytic: {theta_star}')\n",
        "#print(f'y_numeric: {y_numeric}, y_converge: {y_converge} y_analytic: {y_analytic}')\n",
        "print(f'y_theta5: {y_numeric}, y_converge: {y_converge} y_analytic: {y_analytic}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta 5: 0.6446809344000001,1.4195516672000001,1.473319808\n",
            "theta 300: 2.928099443895298,0.47178165138112044,1.2449361314351306\n",
            "analytic: [3.         0.44230769 1.25      ]\n",
            "y_theta5: 12.1623986944, y_converge: 9.021816095106292 y_analytic: 8.961538461538474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE_7_LAsdM-h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}